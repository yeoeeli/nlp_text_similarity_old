{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USING TF-IDF AND COSINE SIMILARITY WITH PARALLEL PROCESSING\n",
    "\n",
    "History on runtime based on parameter and CPU cores\n",
    "- chunk_size = 1000\n",
    "- *cpu_core = -2 OR cpu_core = -3 (no difference in runtime both took ~15min to complete)\n",
    "\n",
    "*n_jobs=-3, it means that joblib will use all but three cores of the CPU for running the tasks. This is because joblib uses negative values of n_jobs to specify the number of cores to keep idle. So, -3 means keep 3 cores idle and use the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import multiprocessing\n",
    "import gc\n",
    "\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx', nrows=100000)\n",
    "df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx', nrows=100000)\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "for word in words_to_remove:\n",
    "    df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "    df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer after data massaging', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party after data massaging', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# concatenate the two columns separately to have same number of rows in each list (difference in the number of rows between the two datasets). This will cause memory error.\n",
    "# use numpy arrays instead of Pandas dataframes to reduce memory usage\n",
    "concatenated_vendor_customer = np.concatenate([df_vendor_customer['Name_Cleaned'].values, np.array([''] * len(df_interested_party))])\n",
    "concatenated_interested_party = np.concatenate([df_interested_party['Interested Party List_Cleaned'].values, np.array([''] * len(df_vendor_customer))])\n",
    "# concatenated_vendor_customer = pd.concat([df_vendor_customer['Name_Cleaned'], pd.Series([''] * len(df_interested_party))])\n",
    "# concatenated_interested_party = pd.concat([df_interested_party['Interested Party List_Cleaned'], pd.Series([''] * len(df_vendor_customer))])\n",
    "\n",
    "\n",
    "# vectorize the 'Name' and 'Interested Party List' columns using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorizer.fit(concatenated_vendor_customer + \" \" + concatenated_interested_party)\n",
    "\n",
    "tfidf_matrix_a = tfidf_vectorizer.transform(df_vendor_customer['Name_Cleaned']).toarray()\n",
    "tfidf_matrix_b = tfidf_vectorizer.transform(df_interested_party['Interested Party List_Cleaned']).toarray()\n",
    "print('Complete: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Define the chunk size and number of jobs for parallel processing\n",
    "chunk_size = 1000\n",
    "num_jobs = -1 # number of CPU cores to use for parallel processing\n",
    "print('Number of CPU cores available:', multiprocessing.cpu_count(), 'Number of CPU cores to use:', num_jobs)\n",
    "\n",
    "\n",
    "# Define a function to compute similarities for a chunk of vendor names\n",
    "def compute_similarities(chunk):\n",
    "    \n",
    "    # Compute the similarity matrix for the chunk\n",
    "    similarity_matrix = cosine_similarity(chunk, tfidf_matrix_b)\n",
    "\n",
    "    # Find the index and value of the interested party name with the highest similarity for each vendor name in the chunk\n",
    "    max_similarities = np.argmax(similarity_matrix, axis=1)\n",
    "    max_similarity_scores = np.max(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Return the corresponding interested party names, similarity scores and interested party source for the chunk\n",
    "    return df_interested_party['Interested Party List_Cleaned'].iloc[max_similarities].values, max_similarity_scores, df_interested_party['Interested Party Source'].iloc[max_similarities].values, df_interested_party['Interested Party List'].iloc[max_similarities].values\n",
    "\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "results = Parallel(n_jobs=num_jobs)(delayed(compute_similarities)(chunk) for chunk in np.array_split(tfidf_matrix_a, len(df_vendor_customer)//chunk_size+1)) # Split the vendor names into chunks and compute similarities in parallel\n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Combine the results for all chunks into a single dataframe\n",
    "corresponding_names = np.concatenate([result[0] for result in results])\n",
    "similarity_scores = np.concatenate([result[1] for result in results])\n",
    "corresponding_interested_party_source = np.concatenate([result[2] for result in results])\n",
    "corresponding_names_original = np.concatenate([result[3] for result in results])\n",
    "\n",
    "\n",
    "# Add the corresponding interested party names and similarity scores to the vendor customer dataframe\n",
    "df_vendor_customer['Corresponding Interested Party Name'] = corresponding_names\n",
    "df_vendor_customer['Cosine Similarity Score'] = similarity_scores \n",
    "df_vendor_customer['Corresponding Interested Party Source'] = corresponding_interested_party_source\n",
    "df_vendor_customer['Corresponding Interested Party Name (Original)'] = corresponding_names_original\n",
    "\n",
    "\n",
    "# Write to excel file\n",
    "df_vendor_customer.to_excel('results_100k.xlsx')\n",
    "print('Complete: Results to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# To free up memory\n",
    "gc.collect()\n",
    "\n",
    "# 4000 rows took 20 seconds with 3 CPU cores\n",
    "# 100,000 rows took 15 minutes with 3 CPU cores - cannot go beyond 100k rows due to insufficient RAM to create all vectors.\n",
    "# 86,197 rows took 7 minutes with 3 CPU cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import multiprocessing\n",
    "import gc\n",
    "\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx', skiprows= range(1, 100001))\n",
    "df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx', nrows=86197)\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "for word in words_to_remove:\n",
    "    df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "    df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer after data massaging', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party after data massaging', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# concatenate the two columns separately to have same number of rows in each list (difference in the number of rows between the two datasets). This will cause memory error.\n",
    "# use numpy arrays instead of Pandas dataframes to reduce memory usage\n",
    "concatenated_vendor_customer = np.concatenate([df_vendor_customer['Name_Cleaned'].values, np.array([''] * len(df_interested_party))])\n",
    "concatenated_interested_party = np.concatenate([df_interested_party['Interested Party List_Cleaned'].values, np.array([''] * len(df_vendor_customer))])\n",
    "# concatenated_vendor_customer = pd.concat([df_vendor_customer['Name_Cleaned'], pd.Series([''] * len(df_interested_party))])\n",
    "# concatenated_interested_party = pd.concat([df_interested_party['Interested Party List_Cleaned'], pd.Series([''] * len(df_vendor_customer))])\n",
    "\n",
    "\n",
    "# vectorize the 'Name' and 'Interested Party List' columns using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorizer.fit(concatenated_vendor_customer + \" \" + concatenated_interested_party)\n",
    "\n",
    "tfidf_matrix_a = tfidf_vectorizer.transform(df_vendor_customer['Name_Cleaned']).toarray()\n",
    "tfidf_matrix_b = tfidf_vectorizer.transform(df_interested_party['Interested Party List_Cleaned']).toarray()\n",
    "print('Complete: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Define the chunk size and number of jobs for parallel processing\n",
    "chunk_size = 1000\n",
    "num_jobs = -3 # number of CPU cores to use for parallel processing\n",
    "print('Number of CPU cores available:', multiprocessing.cpu_count(), '--> Number of CPU cores to use:', num_jobs)\n",
    "\n",
    "\n",
    "# Define a function to compute similarities for a chunk of vendor names\n",
    "def compute_similarities(chunk):\n",
    "    \n",
    "    # Compute the similarity matrix for the chunk\n",
    "    similarity_matrix = cosine_similarity(chunk, tfidf_matrix_b)\n",
    "\n",
    "    # Find the index and value of the interested party name with the highest similarity for each vendor name in the chunk\n",
    "    max_similarities = np.argmax(similarity_matrix, axis=1)\n",
    "    max_similarity_scores = np.max(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Return the corresponding interested party names, similarity scores and interested party source for the chunk\n",
    "    return df_interested_party['Interested Party List_Cleaned'].iloc[max_similarities].values, max_similarity_scores, df_interested_party['Interested Party Source'].iloc[max_similarities].values, df_interested_party['Interested Party List'].iloc[max_similarities].values\n",
    "\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "results = Parallel(n_jobs=num_jobs)(delayed(compute_similarities)(chunk) for chunk in np.array_split(tfidf_matrix_a, len(df_vendor_customer)//chunk_size+1)) # Split the vendor names into chunks and compute similarities in parallel\n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Combine the results for all chunks into a single dataframe\n",
    "corresponding_names = np.concatenate([result[0] for result in results])\n",
    "similarity_scores = np.concatenate([result[1] for result in results])\n",
    "corresponding_interested_party_source = np.concatenate([result[2] for result in results])\n",
    "corresponding_names_original = np.concatenate([result[3] for result in results])\n",
    "\n",
    "\n",
    "# Add the corresponding interested party names and similarity scores to the vendor customer dataframe\n",
    "df_vendor_customer['Corresponding Interested Party Name'] = corresponding_names\n",
    "df_vendor_customer['Cosine Similarity Score'] = similarity_scores \n",
    "df_vendor_customer['Corresponding Interested Party Source'] = corresponding_interested_party_source\n",
    "df_vendor_customer['Corresponding Interested Party Name (Original)'] = corresponding_names_original\n",
    "\n",
    "\n",
    "# Write to excel file\n",
    "df_vendor_customer.to_excel('results_86k.xlsx')\n",
    "print('Complete: Results to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# To free up memory\n",
    "gc.collect()\n",
    "\n",
    "# 4000 rows took 20 seconds with 3 CPU cores\n",
    "# 100,000 rows took 15 minutes with 3 CPU cores - cannot go beyond 100k rows due to insufficient RAM to create all vectors.\n",
    "# 86,197 rows took 7 minutes with 3 CPU cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combined multiple excelfile into single excelfile\n",
    "df1 = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/results_100k.xlsx')\n",
    "df2 = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/results_86k.xlsx')\n",
    "\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "combined_df.to_excel('combined_results.xlsx', index=False)\n",
    "\n",
    "# 177k rows in total - 1min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USING LEVENSHTEIN DISTANCE WITH PARALLEL PROCESSING\n",
    "\n",
    "- 4000 rows with -3 CPU cores, 1000 chunk size took 3min\n",
    "- 186k rows with -3 CPU cores, 1000 chunk size took 182min (3hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using fuzzywuzzy based on Levenshtein Distance\n",
    "\n",
    "import pandas as pd\n",
    "import multiprocessing \n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# Load the dataframes\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx')\n",
    "df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx')\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "for word in words_to_remove:\n",
    "    df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "    df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Define the chunk size and number of jobs for parallel processing\n",
    "chunk_size = 1000\n",
    "num_jobs = -3 # number of CPU cores to use for parallel processing\n",
    "print('Number of CPU cores available:', multiprocessing.cpu_count(), '--> Number of CPU cores to use:', num_jobs)\n",
    "\n",
    "# Define a function to compute similarities for a chunk of vendor names using fuzzywuzzy\n",
    "def compute_similarities_fuzzy(chunk):\n",
    "\n",
    "    # Compute the similarity scores between each vendor name and each interested party name in the chunk\n",
    "    similarities = [[fuzz.token_sort_ratio(vendor, party) for party in df_interested_party['Interested Party List_Cleaned']] for vendor in chunk]\n",
    "\n",
    "    # Find the index and value of the interested party name with the highest similarity score for each vendor name in the chunk\n",
    "    max_similarities = np.argmax(similarities, axis=1)\n",
    "    max_similarity_scores = np.max(similarities, axis=1)\n",
    "    \n",
    "    # Return the corresponding interested party names, similarity scores and interested party source for the chunk\n",
    "    return df_interested_party['Interested Party List_Cleaned'].iloc[max_similarities].values, max_similarity_scores, df_interested_party['Interested Party Source'].iloc[max_similarities].values, df_interested_party['Interested Party List'].iloc[max_similarities].values\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "results = Parallel(n_jobs=num_jobs)(delayed(compute_similarities_fuzzy)(chunk) for chunk in np.array_split(df_vendor_customer['Name_Cleaned'].values, len(df_vendor_customer)//chunk_size+1)) # Split the vendor names into chunks and compute similarities in parallel using fuzzywuzzy\n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Combine the results for all chunks into a single dataframe\n",
    "corresponding_names = np.concatenate([result[0] for result in results])\n",
    "similarity_scores = np.concatenate([result[1] for result in results])\n",
    "corresponding_interested_party_source = np.concatenate([result[2] for result in results])\n",
    "corresponding_names_original = np.concatenate([result[3] for result in results])\n",
    "\n",
    "\n",
    "# Add the corresponding interested party names and similarity scores to the vendor customer dataframe\n",
    "df_vendor_customer['Corresponding Interested Party Name'] = corresponding_names\n",
    "df_vendor_customer['Fuzzy Score'] = similarity_scores \n",
    "df_vendor_customer['Corresponding Interested Party Source'] = corresponding_interested_party_source\n",
    "df_vendor_customer['Corresponding Interested Party Name (Original)'] = corresponding_names_original\n",
    "\n",
    "\n",
    "# Write to excel file\n",
    "df_vendor_customer.to_excel('result_fuzzywuzzy_PP.xlsx', index=False)\n",
    "print('Complete: Results to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# 4000 rows with -3 CPU cores, 1000 chunk size took 3min\n",
    "# 186k rows with -3 CPU cores, 1000 chunk size took 182min (3hours)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
