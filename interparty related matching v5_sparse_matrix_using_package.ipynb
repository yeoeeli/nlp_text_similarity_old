{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just checking how many rows are there in the source excelfile...\n",
      "Number of rows in raw_df_vendor_customer 186197\n",
      "Number of rows in raw_df_interested_party 3587\n",
      "ANALYSES BEGINS! :)\n",
      "Complete: Data loaded into DF 2023-04-17 11:55:50\n",
      "Number of rows in df_vendor_customer 186197\n",
      "Number of rows in df_interested_party 3587\n",
      "Start: Data Massaging 2023-04-17 11:55:50\n",
      "Complete: Data Massaging 2023-04-17 11:55:51\n",
      "Number of rows in df_vendor_customer after data massaging 176158\n",
      "Number of rows in df_interested_party after data massaging 3587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('./Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('./Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('./Vendor Customer.xlsx')\n",
    "df_interested_party = pd.read_excel('./Interested Parties.xlsx')\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "# for word in words_to_remove:\n",
    "#     df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "#     df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer after data massaging', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party after data massaging', len(df_interested_party))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Vectorisation 2023-04-17 11:41:23\n",
      "Vendor-Customer matrix shape: (176158, 70792)\n",
      "Interested Party matrix shape: (3587, 70792)\n",
      "Similarity Matrix Shape: (176158, 3587)\n",
      "End: Vectorisation 2023-04-17 11:41:25\n",
      "Number of CPU cores available: 4 \n",
      "Number of CPU cores to use: -2 \n",
      "Batch Size: 1000\n",
      "Start: Parallel Processing 2023-04-17 11:41:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-2)]: Done 2006 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-2)]: Done 7006 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-2)]: Done 12006 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-2)]: Done 19006 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-2)]: Done 26006 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-2)]: Done 35006 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-2)]: Done 44006 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-2)]: Done 55006 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-2)]: Done 66006 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-2)]: Done 79006 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-2)]: Done 92006 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-2)]: Done 107006 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-2)]: Done 122006 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done 139006 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done 156006 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-2)]: Done 173148 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-2)]: Done 175426 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-2)]: Done 176158 out of 176158 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End: Parallel Processing 2023-04-17 11:44:49\n",
      "Start: Writing to Excel 2023-04-17 11:44:49\n",
      "End: Writing to Excel 2023-04-17 11:45:13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "\n",
    "print('Start: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Define the TfidfVectorizer and fit it on the combined text column of both dataframes\n",
    "vectorizer = TfidfVectorizer() # text extraction technique for text\n",
    "vectorizer.fit(pd.concat([df_vendor_customer['Name_Cleaned'], df_interested_party['Interested Party List_Cleaned']]))\n",
    "\n",
    "# Create a sparse matrix representation of the text column for each dataframe, TfidfVectorizer uses sparse matrix by default. To convert to dense matrix, use .toarray(). \n",
    "vendor_customer_matrix = vectorizer.transform(df_vendor_customer['Name_Cleaned'])\n",
    "interested_party_matrix = vectorizer.transform(df_interested_party['Interested Party List_Cleaned'])\n",
    "print('Vendor-Customer matrix shape:', vendor_customer_matrix.shape)\n",
    "print('Interested Party matrix shape:', interested_party_matrix.shape)\n",
    "\n",
    "\n",
    "\"\"\" Initialize the similarity matrix as a sparse matrix. \n",
    "# The .shape attribute is a property of a NumPy array or a sparse matrix in Python. It returns a tuple containing the dimensions of the array or matrix, in the format (number of rows, number of columns).\n",
    "# .shape[0] represents to get the number of rows in both vendor_customer_matrix and interested_party_matrix. Please note the matrix is based on a single column, and therefore .shape[0] would make sense\n",
    "# similarity matrix to have the same number of rows as the vendor_customer_matrix and the same number of columns as the interested_party_matrix.\n",
    "# The lil_matrix() function is used to create a sparse matrix in the LIL (List of Lists) format. \n",
    "# The LIL format is a way of storing sparse matrices in memory that makes it easy to add new elements to the matrix one at a time. It's a good choice if you want to build up a sparse matrix incrementally.\n",
    "\"\"\"\n",
    "similarity_matrix = sp.lil_matrix((vendor_customer_matrix.shape[0], interested_party_matrix.shape[0]))\n",
    "print('Similarity Matrix Shape:', similarity_matrix.shape)\n",
    "\n",
    "print('End: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(vendor_customer_matrix, interested_party_matrix, n_jobs, batch_size):\n",
    "    \n",
    "    # Compute the cosine similarities between the rows of the two matrices\n",
    "    # This line calculates the cosine similarity between each row of the vendor-customer matrix and each row of the interested party matrix, resulting in a matrix of similarity scores.\n",
    "    similarity_scores = cosine_similarity(vendor_customer_matrix, interested_party_matrix)\n",
    "        \n",
    "    # Find the most similar interested party and its similarity score for each row of the vendor_customer_matrix\n",
    "    def find_most_similar(row_idx):\n",
    "        \n",
    "        # This line finds the index of the row in the interested party matrix that has the highest cosine similarity score with the current row of the vendor-customer matrix.\n",
    "        max_idx = np.argmax(similarity_scores[row_idx]) \n",
    "        \n",
    "        # This line retrieves the highest similarity score for the current row of the vendor-customer matrix.\n",
    "        max_similarity_score = similarity_scores[row_idx, max_idx] \n",
    "        \n",
    "        # This line retrieves the \"Interested Party List_Cleaned\" column value from the row in the interested party dataframe that corresponds to the index max_idx found above.\n",
    "        most_similar_Interested_Party_List_Cleaned = df_interested_party.iloc[max_idx]['Interested Party List_Cleaned'] \n",
    "        corresponding_interested_party_source = df_interested_party.iloc[max_idx]['Interested Party Source'] \n",
    "        \n",
    "        # to return all relevant columns to put into a dataframe\n",
    "        return pd.Series({\n",
    "            'Code': df_vendor_customer.iloc[row_idx]['Code'],\n",
    "            'Name_Cleaned': df_vendor_customer.iloc[row_idx]['Name_Cleaned'],\n",
    "            'most_similar_Interested_Party_List_Cleaned': most_similar_Interested_Party_List_Cleaned,\n",
    "            'Corresponding Interested Party Source': corresponding_interested_party_source,\n",
    "            'similarity_score': max_similarity_score\n",
    "        })\n",
    "\n",
    "    \"\"\"# Use parallel processing to speed up computation\n",
    "    # Creates a sequence of delayed function calls, each of which corresponds to a row of vendor_customer_matrix.\n",
    "    # For each row index row_idx in the range range(vendor_customer_matrix.shape[0]), it creates a delayed object, which is a callable that will execute the find_most_similar function with the argument row_idx when called. \n",
    "    # The delayed function is a utility function provided by the joblib library that creates a wrapper function around a given function, making it suitable for parallel execution.\n",
    "    \"\"\"\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10, batch_size=batch_size)(delayed(find_most_similar)(row_idx) for row_idx in range(vendor_customer_matrix.shape[0]))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# Define the batch size and number of jobs (ie CPU Cores) for parallel processing\n",
    "n_jobs = -2\n",
    "batch_size = 1000\n",
    "print('Number of CPU cores available:', joblib.cpu_count(), '\\nNumber of CPU cores to use:', n_jobs, '\\nBatch Size:', batch_size)\n",
    "\n",
    "\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Call the function and store the resulting dataframe in a variable\n",
    "result_df = compute_cosine_similarity(vendor_customer_matrix, interested_party_matrix, n_jobs, batch_size)\n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "print('Start: Writing to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "result_df.head()\n",
    "result_df.to_excel('results_sparse.xlsx')\n",
    "print('End: Writing to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# shape size: (176158, 70792)\n",
    "# runtime: 3min\n",
    "# batchsize: 1000, CPU cores: -2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
