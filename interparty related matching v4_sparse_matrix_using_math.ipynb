{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Sparse Matrix to perform vectorsation\n",
    "\n",
    "What is sparse matrix?\n",
    "  - In a sparse matrix, most of the elements have a value of zero, which means that the matrix contains a lot of redundant information. Therefore, operations on sparse matrices can be significantly faster than on dense matrices because the computations can skip over the zero elements. Sparse matrices are often used in situations where memory usage is a concern, such as in large-scale scientific simulations, machine learning, and data processing.\n",
    "\n",
    "How is it different from dense matrix?\n",
    "  - Dense matrices have a high density of non-zero elements, and therefore, they contain a lot of useful information. Operations on dense matrices can be more computationally expensive than on sparse matrices because every element needs to be processed. Dense matrices are commonly used in situations where precision is critical, such as in numerical analysis and linear algebra.\n",
    "\n",
    "What is the challenges in coding for sparse matrix?\n",
    "  - Coding for sparse matrices can be more complex than coding for dense matrices due to the additional data structures and algorithms needed to handle the sparsity. \n",
    "  - Working with sparse matrices requires understanding the underlying data structures, such as Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) formats, and using specialized algorithms, such as sparse matrix multiplication or sparse matrix factorization. These additional complexities can make coding for sparse matrices more challenging.\n",
    "  - In contrast, dense matrices can be more straightforward to code since they use standard data structures such as arrays and require standard algorithms such as matrix multiplication and inversion. But requires higher memory and computational resources.\n",
    "\n",
    "\n",
    "Runtime results\n",
    "  - For the same set of the data and data massaging performed.\n",
    "  - Does not require to run in different batches (due to lesser memory and computational resources required), unlike in script done in _dense_matrix.ipynb\n",
    "  - 12 mins for ~179k rows to match again 3.6k rows for 3 concurrent CPU workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just checking how many rows are there in the source excelfile...\n",
      "Number of rows in raw_df_vendor_customer 186197\n",
      "Number of rows in raw_df_interested_party 3587\n",
      "ANALYSES BEGINS! :)\n",
      "Complete: Data loaded into DF 2023-04-15 17:51:48\n",
      "Number of rows in df_vendor_customer 186197\n",
      "Number of rows in df_interested_party 3587\n",
      "Start: Data Massaging 2023-04-15 17:51:48\n",
      "Complete: Data Massaging 2023-04-15 17:51:49\n",
      "Number of rows in df_vendor_customer after data massaging 176158\n",
      "Number of rows in df_interested_party after data massaging 3587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('./Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('./Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('./Vendor Customer.xlsx')\n",
    "df_interested_party = pd.read_excel('./Interested Parties.xlsx')\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "# for word in words_to_remove:\n",
    "#     df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "#     df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer after data massaging', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party after data massaging', len(df_interested_party))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Vectorisation 2023-04-15 17:51:49\n",
      "End: Vectorisation 2023-04-15 17:51:52\n",
      "Number of CPU cores available: 4 \n",
      "Number of CPU cores to use: -2 \n",
      "Batch Size: 1000\n",
      "Vendor-Customer matrix shape: (176158, 70792)\n",
      "Interested Party matrix shape: (3587, 70792)\n",
      "Start: Parallel Processing 2023-04-15 17:51:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-2)]: Done 2006 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-2)]: Done 7006 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-2)]: Done 12006 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=-2)]: Done 19006 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-2)]: Done 26006 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done 35006 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-2)]: Done 44006 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-2)]: Done 55006 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-2)]: Done 66006 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-2)]: Done 79006 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-2)]: Done 92006 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-2)]: Done 107006 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-2)]: Done 122006 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-2)]: Done 139006 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-2)]: Done 156006 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-2)]: Done 172219 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-2)]: Done 175426 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-2)]: Done 176158 out of 176158 | elapsed: 14.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End: Parallel Processing 2023-04-15 18:06:21\n",
      "Start: Writing to Excel 2023-04-15 18:06:21\n",
      "End: Writing to Excel 2023-04-15 18:06:37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "\n",
    "print('Start: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Define the TfidfVectorizer and fit it on the combined text column of both dataframes\n",
    "vectorizer = TfidfVectorizer() # text extraction technique for text\n",
    "vectorizer.fit(pd.concat([df_vendor_customer['Name_Cleaned'], df_interested_party['Interested Party List_Cleaned']]))\n",
    "\n",
    "# Create a sparse matrix representation of the text column for each dataframe, TfidfVectorizer uses sparse matrix by default. To convert to dense matrix, use .toarray(). \n",
    "vendor_customer_matrix = vectorizer.transform(df_vendor_customer['Name_Cleaned'])\n",
    "interested_party_matrix = vectorizer.transform(df_interested_party['Interested Party List_Cleaned'])\n",
    "print('Vendor-Customer matrix shape:', vendor_customer_matrix.shape)\n",
    "print('Interested Party matrix shape:', interested_party_matrix.shape)\n",
    "\n",
    "\n",
    "\"\"\" Initialize the similarity matrix as a sparse matrix. \n",
    "# The .shape attribute is a property of a NumPy array or a sparse matrix in Python. It returns a tuple containing the dimensions of the array or matrix, in the format (number of rows, number of columns).\n",
    "# .shape[0] represents to get the number of rows in both vendor_customer_matrix and interested_party_matrix. Please note the matrix is based on a single column, and therefore .shape[0] would make sense\n",
    "# similarity matrix to have the same number of rows as the vendor_customer_matrix and the same number of columns as the interested_party_matrix.\n",
    "\"\"\"\n",
    "similarity_matrix = sp.lil_matrix((vendor_customer_matrix.shape[0], interested_party_matrix.shape[0]))\n",
    "print('Similarity Matrix Shape:', similarity_matrix)\n",
    "\n",
    "print('End: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(vendor_customer_matrix, interested_party_matrix, n_jobs, batch_size):\n",
    "    \"\"\"Compute cosine similarity between two sparse matrices using Joblib's parallel processing.\n",
    "\n",
    "    Args:\n",
    "        vendor_customer_matrix (scipy.sparse.csr_matrix): The sparse matrix for the 'Name_Cleaned' column in df_vendor_customer.\n",
    "        interested_party_matrix (scipy.sparse.csr_matrix): The sparse matrix for the 'Interested Party List_Cleaned' column in df_interested_party.\n",
    "        n_jobs (int): The number of parallel jobs to run. Defaults to -1, which uses all available CPUs (not receommend, use -2 or -3 ideally)\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe containing the most similar row and its score for each row in vendor_customer_matrix.\n",
    "    \"\"\"\n",
    "    # Normalize the rows of the two matrices\n",
    "    norm_1 = np.sqrt(np.asarray(vendor_customer_matrix.power(2).sum(axis=1)).flatten())\n",
    "    norm_2 = np.sqrt(np.asarray(interested_party_matrix.power(2).sum(axis=1)).flatten())\n",
    "\n",
    "    norm_1[norm_1 == 0] = 1\n",
    "    norm_2[norm_2 == 0] = 1\n",
    "\n",
    "    normalized_matrix_1 = vendor_customer_matrix.multiply(1 / norm_1[:, np.newaxis])\n",
    "    normalized_matrix_2 = interested_party_matrix.multiply(1 / norm_2[:, np.newaxis])\n",
    "\n",
    "    # Compute the dot product between the rows of the two matrices\n",
    "    def compute_dot_product(row_idx):\n",
    "        \n",
    "        # similarity_scores = normalized_matrix_1.getrow(row_idx).dot(normalized_matrix_2.T).toarray().flatten()\n",
    "        # max_idx = np.argmax(similarity_scores)\n",
    "        # return pd.Series({'most_similar_row': max_idx, 'similarity_score': similarity_scores[max_idx]})\n",
    "        \n",
    "        similarity_scores = normalized_matrix_1.getrow(row_idx).dot(normalized_matrix_2.T).toarray().flatten()\n",
    "        \n",
    "        max_idx = np.argmax(similarity_scores)\n",
    "        most_similar_Interested_Party_List_Cleaned = df_interested_party.iloc[max_idx]['Interested Party List_Cleaned']\n",
    "        similarity_score = similarity_scores[max_idx]\n",
    "        row = df_vendor_customer.iloc[row_idx]\n",
    "        \n",
    "        return pd.Series({'Name_Cleaned': row['Name_Cleaned'], 'most_similar_Interested Party List_Cleaned': most_similar_Interested_Party_List_Cleaned, 'similarity_score': similarity_score})\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10, batch_size=batch_size)(delayed(compute_dot_product)(row_idx) for row_idx in range(normalized_matrix_1.shape[0]))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Define the batch size and number of jobs (ie CPU Cores) for parallel processing\n",
    "n_jobs = -2\n",
    "batch_size = 1000\n",
    "print('Number of CPU cores available:', joblib.cpu_count(), '\\nNumber of CPU cores to use:', n_jobs, '\\nBatch Size:', batch_size)\n",
    "\n",
    "\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Call the function and store the resulting dataframe in a variable\n",
    "result_df = compute_cosine_similarity(vendor_customer_matrix, interested_party_matrix, n_jobs, batch_size)\n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "print('Start: Writing to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# result_df.head()\n",
    "result_df.to_excel('results_sparse.xlsx')\n",
    "print('End: Writing to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Vectorisation 2023-04-15 18:09:50\n",
      "End: Vectorisation 2023-04-15 18:09:52\n",
      "Number of CPU cores available: 4 \n",
      "Number of CPU cores to use: -2 \n",
      "Batch Size: 1000\n",
      "Vendor-Customer matrix shape: (176158, 70792)\n",
      "Interested Party matrix shape: (3587, 70792)\n",
      "Start: Parallel Processing 2023-04-15 18:09:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-2)]: Done 2006 tasks      | elapsed: 66.9min\n",
      "[Parallel(n_jobs=-2)]: Done 7006 tasks      | elapsed: 117.9min\n",
      "[Parallel(n_jobs=-2)]: Done 12006 tasks      | elapsed: 147.9min\n",
      "[Parallel(n_jobs=-2)]: Done 19006 tasks      | elapsed: 232.7min\n",
      "[Parallel(n_jobs=-2)]: Done 26006 tasks      | elapsed: 334.5min\n",
      "[Parallel(n_jobs=-2)]: Done 35006 tasks      | elapsed: 426.7min\n",
      "[Parallel(n_jobs=-2)]: Done 44006 tasks      | elapsed: 547.1min\n",
      "[Parallel(n_jobs=-2)]: Done 55006 tasks      | elapsed: 645.3min\n",
      "[Parallel(n_jobs=-2)]: Done 66006 tasks      | elapsed: 788.8min\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "print('Start: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Define the TfidfVectorizer and fit it on the combined text column of both dataframes\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(pd.concat([df_vendor_customer['Name_Cleaned'], df_interested_party['Interested Party List_Cleaned']]))\n",
    "\n",
    "# Create a sparse matrix representation of the text column for each dataframe\n",
    "vendor_customer_matrix = vectorizer.transform(df_vendor_customer['Name_Cleaned'])\n",
    "interested_party_matrix = vectorizer.transform(df_interested_party['Interested Party List_Cleaned'])\n",
    "\n",
    "# Initialize the similarity matrix as a sparse matrix\n",
    "similarity_matrix = sp.lil_matrix((vendor_customer_matrix.shape[0], interested_party_matrix.shape[0]))\n",
    "\n",
    "print('End: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def compute_levenshtein_similarity(vendor_customer_matrix, interested_party_matrix, n_jobs, batch_size):\n",
    "    \"\"\"Compute Levenshtein similarity between two sparse matrices using Joblib's parallel processing.\n",
    "\n",
    "    Args:\n",
    "        vendor_customer_matrix (scipy.sparse.csr_matrix): The sparse matrix for the 'Name_Cleaned' column in df_vendor_customer.\n",
    "        interested_party_matrix (scipy.sparse.csr_matrix): The sparse matrix for the 'Interested Party List_Cleaned' column in df_interested_party.\n",
    "        n_jobs (int): The number of parallel jobs to run. Defaults to -1, which uses all available CPUs.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe containing the most similar row and its score for each row in vendor_customer_matrix.\n",
    "    \"\"\"\n",
    "    # Compute the Levenshtein distance between the rows of the two matrices\n",
    "    def compute_levenshtein_distance(row_idx):\n",
    "        \n",
    "        similarity_scores = [fuzz.token_set_ratio(vendor_customer_matrix[row_idx], interested_party_matrix[idx]) for idx in range(interested_party_matrix.shape[0])]\n",
    "        \n",
    "        max_idx = np.argmax(similarity_scores)\n",
    "        most_similar_Interested_Party_List_Cleaned = df_interested_party.iloc[max_idx]['Interested Party List_Cleaned']\n",
    "        similarity_score = similarity_scores[max_idx]\n",
    "        row = df_vendor_customer.iloc[row_idx]\n",
    "        \n",
    "        return pd.Series({'Name_Cleaned': row['Name_Cleaned'], 'most_similar_Interested Party List_Cleaned': most_similar_Interested_Party_List_Cleaned, 'similarity_score': similarity_score})\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10, batch_size=batch_size)(delayed(compute_levenshtein_distance)(row_idx) for row_idx in range(vendor_customer_matrix.shape[0]))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define the batch size and number of jobs (ie CPU Cores) for parallel processing\n",
    "n_jobs = -2\n",
    "batch_size = 1000\n",
    "print('Number of CPU cores available:', joblib.cpu_count(), '\\nNumber of CPU cores to use:', n_jobs, '\\nBatch Size:', batch_size)\n",
    "\n",
    "print('Vendor-Customer matrix shape:', vendor_customer_matrix.shape)\n",
    "print('Interested Party matrix shape:', interested_party_matrix.shape)\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Call the function and store the resulting dataframe in a variable\n",
    "result_df = compute_levenshtein_similarity(vendor_customer_matrix, interested_party_matrix, n_jobs, batch_size)\n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "print('Start: Writing to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# result_df.head()\n",
    "result_df.to_excel('results_sparse.xlsx')\n",
    "print('End: Writing to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
