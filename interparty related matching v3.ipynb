{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USING TF-IDF AND COSINE SIMILARITY WITH PARALLEL PROCESSING\n",
    "\n",
    "History on runtime based on parameter and CPU cores\n",
    "- chunk_size = 1000\n",
    "- *cpu_core = -2 OR cpu_core = -3 (no difference in runtime both took ~15min to complete)\n",
    "- seems like changing the above parameter doesn't help in the runtime\n",
    "\n",
    "*n_jobs=-3, it means that joblib will use all but three cores of the CPU for running the tasks. This is because joblib uses negative values of n_jobs to specify the number of cores to keep idle. So, -3 means keep 3 cores idle and use the rest.\n",
    "\n",
    "seems like changing the number of CPU cores doesnt affect the performance speed by changing the chunk size does. \n",
    "\n",
    "Other testing performed:\n",
    "  - Problem: Want the output to show how is the CPU core distribution for each chunk size. Unfortunately, backend='loky' does not supports the display but backend='threading' does, backend='multiprocessing' just doesnt work https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html. Bear in mind that Python do not allow parallel processing on threads (read GIL - https://wiki.python.org/moin/GlobalInterpreterLock, https://towardsdatascience.com/python-gil-e63f18a08c65), but only parallel processing **on cores**\n",
    "    - How to overcome? called verbose function in Parallel which display how many concurrent workers (ie cores) it is being used. Alternatively go to Task Manager > Details > python.exe > you should see the same number python.exe as the amt of CPU cores that you indicate. \n",
    "  - Tried using Dask - the performance seems to be similar for 4000 rows.\n",
    "    - Dask have a diagnostic dashboard, but my laptop doesnt seems to be displaying the URL to access the dashboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn based on cosine similarity and tf-idf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import multiprocessing\n",
    "import gc\n",
    "\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx', nrows=100000)\n",
    "df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx', nrows=100000)\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "for word in words_to_remove:\n",
    "    df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "    df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer after data massaging', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party after data massaging', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# concatenate the two columns separately to have same number of rows in each list (difference in the number of rows between the two datasets). This will cause memory error.\n",
    "# use numpy arrays instead of Pandas dataframes to reduce memory usage\n",
    "concatenated_vendor_customer = np.concatenate([df_vendor_customer['Name_Cleaned'].values, np.array([''] * len(df_interested_party))])\n",
    "concatenated_interested_party = np.concatenate([df_interested_party['Interested Party List_Cleaned'].values, np.array([''] * len(df_vendor_customer))])\n",
    "# concatenated_vendor_customer = pd.concat([df_vendor_customer['Name_Cleaned'], pd.Series([''] * len(df_interested_party))])\n",
    "# concatenated_interested_party = pd.concat([df_interested_party['Interested Party List_Cleaned'], pd.Series([''] * len(df_vendor_customer))])\n",
    "\n",
    "\n",
    "# vectorize the 'Name' and 'Interested Party List' columns using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer() # text extraction technique for text\n",
    "\n",
    "tfidf_vectorizer.fit(concatenated_vendor_customer + \" \" + concatenated_interested_party) # Learn vocabulary and idf from training set. The fit() method of TfidfVectorizer computes the inverse document frequency (IDF) weights for each word in the vocabulary of the text data and fits the vectorizer to the data, which means that it learns the vocabulary of the data and computes the IDF weights for each word.\n",
    "\n",
    "# The transform() method takes as input a list of text instances and returns a matrix of numerical values representing the input text instances in the same vector space as the training data.\n",
    "tfidf_matrix_a = tfidf_vectorizer.transform(df_vendor_customer['Name_Cleaned']).toarray()\n",
    "tfidf_matrix_b = tfidf_vectorizer.transform(df_interested_party['Interested Party List_Cleaned']).toarray()\n",
    "print('Complete: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Define the chunk size and number of jobs for parallel processing\n",
    "chunk_size = 1000\n",
    "num_jobs = -2 # number of CPU cores to use for parallel processing\n",
    "print('Number of CPU cores available:', multiprocessing.cpu_count(), 'Number of CPU cores to use:', num_jobs)\n",
    "\n",
    "\n",
    "# Define a function to compute similarities for a chunk of vendor names\n",
    "def compute_similarities(chunk):\n",
    "    \n",
    "    # Compute the similarity matrix for the chunk\n",
    "    similarity_matrix = cosine_similarity(chunk, tfidf_matrix_b)\n",
    "\n",
    "    # Find the index and value of the interested party name with the highest similarity for each vendor name in the chunk\n",
    "    max_similarities = np.argmax(similarity_matrix, axis=1)\n",
    "    max_similarity_scores = np.max(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Return the corresponding interested party names, similarity scores and interested party source for the chunk\n",
    "    return df_interested_party['Interested Party List_Cleaned'].iloc[max_similarities].values, max_similarity_scores, df_interested_party['Interested Party Source'].iloc[max_similarities].values, df_interested_party['Interested Party List'].iloc[max_similarities].values\n",
    "\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Split the vendor names into chunks and compute similarities in parallel\n",
    "# np.array_split divides the data into equally sized chunks, which are then processed in parallel by Parallel with n_jobs number of CPU cores\n",
    "results = Parallel(n_jobs=num_jobs, verbose=10, backend='loky')(delayed(compute_similarities)(chunk) for chunk in np.array_split(tfidf_matrix_a, len(df_vendor_customer)//chunk_size+1)) \n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Combine the results for all chunks into a single dataframe\n",
    "corresponding_names = np.concatenate([result[0] for result in results])\n",
    "similarity_scores = np.concatenate([result[1] for result in results])\n",
    "corresponding_interested_party_source = np.concatenate([result[2] for result in results])\n",
    "corresponding_names_original = np.concatenate([result[3] for result in results])\n",
    "\n",
    "\n",
    "# Add the corresponding interested party names and similarity scores to the vendor customer dataframe\n",
    "df_vendor_customer['Corresponding Interested Party Name'] = corresponding_names\n",
    "df_vendor_customer['Cosine Similarity Score'] = similarity_scores \n",
    "df_vendor_customer['Corresponding Interested Party Source'] = corresponding_interested_party_source\n",
    "df_vendor_customer['Corresponding Interested Party Name (Original)'] = corresponding_names_original\n",
    "\n",
    "\n",
    "# Write to excel file\n",
    "df_vendor_customer.to_excel('results_100k.xlsx')\n",
    "print('Complete: Results to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# To free up memory\n",
    "gc.collect()\n",
    "\n",
    "# 4000 rows 4 CPU core, chunk_size = 100: 46 seconds\n",
    "# 4000 rows 4 CPU core, chunk_size = 1000: 37 seconds\n",
    "# 4000 rows -3 CPU core, chunk_size = 1000: 36 seconds\n",
    "# 4000 rows -3 CPU core, chunk_size = 10000: 36 seconds\n",
    "\n",
    "# 100,000 rows took 15 minutes with -3 CPU cores - cannot go beyond 100k rows due to insufficient RAM to create all vectors.\n",
    "# 86,197 rows took 7 minutes with -3 CPU cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just checking how many rows are there in the source excelfile...\n",
      "Number of rows in raw_df_vendor_customer 186197\n",
      "Number of rows in raw_df_interested_party 3587\n",
      "ANALYSES BEGINS! :)\n",
      "Complete: Data loaded into DF 2023-04-14 13:41:26\n",
      "Number of rows in df_vendor_customer 86197\n",
      "Number of rows in df_interested_party 3587\n",
      "Start: Data Massaging 2023-04-14 13:41:26\n",
      "Complete: Data Massaging 2023-04-14 13:41:29\n",
      "Number of rows in df_vendor_customer after data massaging 76158\n",
      "Number of rows in df_interested_party after data massaging 3587\n",
      "Start: Vectorisation 2023-04-14 13:41:29\n",
      "Complete: Vectorisation 2023-04-14 13:41:31\n",
      "Number of CPU cores available: 4 Number of CPU cores to use: -2\n",
      "Start: Parallel Processing 2023-04-14 13:41:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   2 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-2)]: Done   7 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-2)]: Done  12 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-2)]: Done  26 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-2)]: Done  35 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-2)]: Done  55 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-2)]: Done  66 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-2)]: Done  77 out of  77 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End: Parallel Processing 2023-04-14 13:46:59\n",
      "Complete: Results to Excel 2023-04-14 13:47:22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sklearn based on cosine similarity and tf-idf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import multiprocessing\n",
    "import gc\n",
    "\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx', skiprows= range(1, 100001))\n",
    "df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx', nrows=86197)\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "for word in words_to_remove:\n",
    "    df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "    df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer after data massaging', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party after data massaging', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# concatenate the two columns separately to have same number of rows in each list (difference in the number of rows between the two datasets). This will cause memory error.\n",
    "# use numpy arrays instead of Pandas dataframes to reduce memory usage\n",
    "concatenated_vendor_customer = np.concatenate([df_vendor_customer['Name_Cleaned'].values, np.array([''] * len(df_interested_party))])\n",
    "concatenated_interested_party = np.concatenate([df_interested_party['Interested Party List_Cleaned'].values, np.array([''] * len(df_vendor_customer))])\n",
    "# concatenated_vendor_customer = pd.concat([df_vendor_customer['Name_Cleaned'], pd.Series([''] * len(df_interested_party))])\n",
    "# concatenated_interested_party = pd.concat([df_interested_party['Interested Party List_Cleaned'], pd.Series([''] * len(df_vendor_customer))])\n",
    "\n",
    "\n",
    "# vectorize the 'Name' and 'Interested Party List' columns using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer() # text extraction technique for text\n",
    "\n",
    "tfidf_vectorizer.fit(concatenated_vendor_customer + \" \" + concatenated_interested_party) # Learn vocabulary and idf from training set. The fit() method of TfidfVectorizer computes the inverse document frequency (IDF) weights for each word in the vocabulary of the text data and fits the vectorizer to the data, which means that it learns the vocabulary of the data and computes the IDF weights for each word.\n",
    "\n",
    "# The transform() method takes as input a list of text instances and returns a matrix of numerical values representing the input text instances in the same vector space as the training data.\n",
    "tfidf_matrix_a = tfidf_vectorizer.transform(df_vendor_customer['Name_Cleaned']).toarray()\n",
    "tfidf_matrix_b = tfidf_vectorizer.transform(df_interested_party['Interested Party List_Cleaned']).toarray()\n",
    "print('Complete: Vectorisation', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Define the chunk size and number of jobs for parallel processing\n",
    "chunk_size = 1000\n",
    "num_jobs = -2 # number of CPU cores to use for parallel processing\n",
    "print('Number of CPU cores available:', multiprocessing.cpu_count(), 'Number of CPU cores to use:', num_jobs)\n",
    "\n",
    "\n",
    "# Define a function to compute similarities for a chunk of vendor names\n",
    "def compute_similarities(chunk):\n",
    "    \n",
    "    # Compute the similarity matrix for the chunk\n",
    "    similarity_matrix = cosine_similarity(chunk, tfidf_matrix_b)\n",
    "\n",
    "    # Find the index and value of the interested party name with the highest similarity for each vendor name in the chunk\n",
    "    max_similarities = np.argmax(similarity_matrix, axis=1)\n",
    "    max_similarity_scores = np.max(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Return the corresponding interested party names, similarity scores and interested party source for the chunk\n",
    "    return df_interested_party['Interested Party List_Cleaned'].iloc[max_similarities].values, max_similarity_scores, df_interested_party['Interested Party Source'].iloc[max_similarities].values, df_interested_party['Interested Party List'].iloc[max_similarities].values\n",
    "\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Split the vendor names into chunks and compute similarities in parallel\n",
    "# np.array_split divides the data into equally sized chunks, which are then processed in parallel by Parallel with n_jobs number of CPU cores\n",
    "results = Parallel(n_jobs=num_jobs, verbose=10, backend='loky')(delayed(compute_similarities)(chunk) for chunk in np.array_split(tfidf_matrix_a, len(df_vendor_customer)//chunk_size+1)) \n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Combine the results for all chunks into a single dataframe\n",
    "corresponding_names = np.concatenate([result[0] for result in results])\n",
    "similarity_scores = np.concatenate([result[1] for result in results])\n",
    "corresponding_interested_party_source = np.concatenate([result[2] for result in results])\n",
    "corresponding_names_original = np.concatenate([result[3] for result in results])\n",
    "\n",
    "\n",
    "# Add the corresponding interested party names and similarity scores to the vendor customer dataframe\n",
    "df_vendor_customer['Corresponding Interested Party Name'] = corresponding_names\n",
    "df_vendor_customer['Cosine Similarity Score'] = similarity_scores \n",
    "df_vendor_customer['Corresponding Interested Party Source'] = corresponding_interested_party_source\n",
    "df_vendor_customer['Corresponding Interested Party Name (Original)'] = corresponding_names_original\n",
    "\n",
    "\n",
    "# Write to excel file\n",
    "df_vendor_customer.to_excel('results_86k.xlsx')\n",
    "print('Complete: Results to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# To free up memory\n",
    "gc.collect()\n",
    "\n",
    "# 4000 rows 4 CPU core, chunk_size = 100: 46 seconds\n",
    "# 4000 rows 4 CPU core, chunk_size = 1000: 37 seconds\n",
    "# 4000 rows -3 CPU core, chunk_size = 1000: 36 seconds\n",
    "# 4000 rows -3 CPU core, chunk_size = 10000: 36 seconds\n",
    "\n",
    "# 100,000 rows took 15 minutes with -3 CPU cores - cannot go beyond 100k rows due to insufficient RAM to create all vectors.\n",
    "# 86,197 rows took 7 minutes with -3 CPU cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combined multiple excelfile into single excelfile\n",
    "df1 = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/results_100k.xlsx')\n",
    "df2 = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/results_86k.xlsx')\n",
    "\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "combined_df.to_excel('combined_results.xlsx', index=False)\n",
    "\n",
    "# 177k rows in total - 1min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USING LEVENSHTEIN DISTANCE WITH PARALLEL PROCESSING\n",
    "\n",
    "- 4000 rows with -3 CPU cores, 1000 chunk size took 3min\n",
    "- 186k rows with -3 CPU cores, 1000 chunk size took 182min (3hours), no need to split into different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just checking how many rows are there in the source excelfile...\n",
      "Number of rows in raw_df_vendor_customer 186197\n",
      "Number of rows in raw_df_interested_party 3587\n",
      "ANALYSES BEGINS! :)\n",
      "Complete: Data loaded into DF 2023-04-14 13:50:22\n",
      "Number of rows in df_vendor_customer 4000\n",
      "Number of rows in df_interested_party 3587\n",
      "Start: Data Massaging 2023-04-14 13:50:22\n",
      "Complete: Data Massaging 2023-04-14 13:50:22\n",
      "Number of CPU cores available: 4 --> Number of CPU cores to use: -3\n",
      "Start: Parallel Processing 2023-04-14 13:50:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-3)]: Done   3 out of   5 | elapsed:  2.0min remaining:  1.3min\n",
      "[Parallel(n_jobs=-3)]: Done   5 out of   5 | elapsed:  2.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   5 out of   5 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End: Parallel Processing 2023-04-14 13:53:07\n",
      "Complete: Results to Excel 2023-04-14 13:53:08\n"
     ]
    }
   ],
   "source": [
    "# using fuzzywuzzy based on Levenshtein Distance\n",
    "\n",
    "import pandas as pd\n",
    "import multiprocessing \n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# Load the dataframes\n",
    "print('Just checking how many rows are there in the source excelfile...')\n",
    "raw_df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx')\n",
    "raw_df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx')\n",
    "print('Number of rows in raw_df_vendor_customer', len(raw_df_vendor_customer))\n",
    "print('Number of rows in raw_df_interested_party', len(raw_df_interested_party))\n",
    "\n",
    "\n",
    "print('ANALYSES BEGINS! :)')\n",
    "# loading and reading into dataframe\n",
    "df_vendor_customer = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Vendor Customer.xlsx',nrows=4000)\n",
    "df_interested_party = pd.read_excel('C:/Desktop/Repo/Others/interparty related matching/Interested Parties.xlsx', nrows=4000)\n",
    "print('Complete: Data loaded into DF', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Number of rows in df_vendor_customer', len(df_vendor_customer))\n",
    "print('Number of rows in df_interested_party', len(df_interested_party))\n",
    "\n",
    "\n",
    "print('Start: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# set the index as a column, to be used as a mapping field to join df_vendor_customer\n",
    "df_interested_party = df_interested_party.reset_index().rename(columns={'index': 'index_ID'})\n",
    "\n",
    "# to replace NULL/NaN values with empty strings\n",
    "df_vendor_customer['Name'] = df_vendor_customer['Name'].fillna('')\n",
    "df_interested_party['Interested Party List'] =df_interested_party['Interested Party List'].fillna('')\n",
    "\n",
    "\n",
    "# define a regular expression that matches all non-alphanumeric and non-space characters and remove them\n",
    "pattern = re.compile(r'[^\\w\\s]+')\n",
    "\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "# update strings to all uppercase()\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.upper()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.upper()\n",
    "\n",
    "\n",
    "# define the list of common words to remove, to remove noise (similar to stopwords concept)\n",
    "# create a regular expression pattern that includes word boundaries (\\b) before and after each word in the list of words to remove. This ensures that the str.replace method only removes the word when it appears as a standalone word, and not as a substring of other words.\n",
    "words_to_remove = ['PTE', 'LTD', 'LLC', 'CO', 'SDN', 'BHD', 'PTY LIMITED', 'PTY', 'LIMITED', 'PVT', 'PRIVATE', 'INC', 'LLP', 'COMPANY']\n",
    "pattern = r'\\b(' + '|'.join(words_to_remove) + r')\\b'\n",
    "\n",
    "\n",
    "for word in words_to_remove:\n",
    "    df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "    df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# update strings to remove leading and trailing whitespaces\n",
    "df_vendor_customer['Name_Cleaned'] = df_vendor_customer['Name_Cleaned'].str.strip()\n",
    "df_interested_party['Interested Party List_Cleaned'] = df_interested_party['Interested Party List_Cleaned'].str.strip()\n",
    "\n",
    "# to drop duplicated rows\n",
    "df_vendor_customer = df_vendor_customer.drop_duplicates()\n",
    "df_interested_party = df_interested_party.drop_duplicates()\n",
    "\n",
    "print('Complete: Data Massaging', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Define the chunk size and number of jobs for parallel processing\n",
    "chunk_size = 1000\n",
    "num_jobs = -2 # number of CPU cores to use for parallel processing\n",
    "print('Number of CPU cores available:', multiprocessing.cpu_count(), '--> Number of CPU cores to use:', num_jobs)\n",
    "\n",
    "# Define a function to compute similarities for a chunk of vendor names using fuzzywuzzy\n",
    "def compute_similarities_fuzzy(chunk):\n",
    "\n",
    "    # Compute the similarity scores between each vendor name and each interested party name in the chunk\n",
    "    similarities = [[fuzz.token_sort_ratio(vendor, party) for party in df_interested_party['Interested Party List_Cleaned']] for vendor in chunk]\n",
    "\n",
    "    # Find the index and value of the interested party name with the highest similarity score for each vendor name in the chunk\n",
    "    max_similarities = np.argmax(similarities, axis=1)\n",
    "    max_similarity_scores = np.max(similarities, axis=1)\n",
    "    \n",
    "    # Return the corresponding interested party names, similarity scores and interested party source for the chunk\n",
    "    return df_interested_party['Interested Party List_Cleaned'].iloc[max_similarities].values, max_similarity_scores, df_interested_party['Interested Party Source'].iloc[max_similarities].values, df_interested_party['Interested Party List'].iloc[max_similarities].values\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Split the vendor names into chunks and compute similarities in parallel\n",
    "# np.array_split divides the data into equally sized chunks, which are then processed in parallel by Parallel with n_jobs number of CPU cores\n",
    "results = Parallel(n_jobs=num_jobs, verbose=10)(delayed(compute_similarities_fuzzy)(chunk) for chunk in np.array_split(df_vendor_customer['Name_Cleaned'].values, len(df_vendor_customer)//chunk_size+1)) \n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# Combine the results for all chunks into a single dataframe\n",
    "corresponding_names = np.concatenate([result[0] for result in results])\n",
    "similarity_scores = np.concatenate([result[1] for result in results])\n",
    "corresponding_interested_party_source = np.concatenate([result[2] for result in results])\n",
    "corresponding_names_original = np.concatenate([result[3] for result in results])\n",
    "\n",
    "\n",
    "# Add the corresponding interested party names and similarity scores to the vendor customer dataframe\n",
    "df_vendor_customer['Corresponding Interested Party Name'] = corresponding_names\n",
    "df_vendor_customer['Fuzzy Score'] = similarity_scores \n",
    "df_vendor_customer['Corresponding Interested Party Source'] = corresponding_interested_party_source\n",
    "df_vendor_customer['Corresponding Interested Party Name (Original)'] = corresponding_names_original\n",
    "\n",
    "\n",
    "# Write to excel file\n",
    "df_vendor_customer.to_excel('test.xlsx', index=False)\n",
    "print('Complete: Results to Excel', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# 4000 rows with -3 CPU cores, 1000 chunk size took 3min\n",
    "# 186k rows with -3 CPU cores, 1000 chunk size took 182min (3hours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other Testing Performed\n",
    "\n",
    "  - the codes that were tested out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define a function to compute similarities for a chunk of vendor names\n",
    "def compute_similarities(chunk, index):\n",
    "\n",
    "    # to print the compute progress, but this only works with backend='threading'\n",
    "    print('Processing chunk', index, 'with shape', chunk.shape, 'on CPU core', joblib.cpu_count())\n",
    "\n",
    "    # Compute the similarity matrix for the chunk\n",
    "    similarity_matrix = cosine_similarity(chunk, tfidf_matrix_b)\n",
    "\n",
    "    # Find the index and value of the interested party name with the highest similarity for each vendor name in the chunk\n",
    "    max_similarities = np.argmax(similarity_matrix, axis=1)\n",
    "    max_similarity_scores = np.max(similarity_matrix, axis=1)\n",
    "\n",
    "    \n",
    "    # Return the corresponding interested party names, similarity scores and interested party source for the chunk\n",
    "    return df_interested_party['Interested Party List_Cleaned'].iloc[max_similarities].values, max_similarity_scores, df_interested_party['Interested Party Source'].iloc[max_similarities].values, df_interested_party['Interested Party List'].iloc[max_similarities].values\n",
    "\n",
    "\n",
    "print('Start: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "# Split the vendor names into chunks and compute similarities in parallel\n",
    "# np.array_split divides the data into equally sized chunks, which are then processed in parallel by Parallel with n_jobs number of CPU cores\n",
    "\n",
    "# to display the compute progress (additional parameter of index)\n",
    "results = Parallel(n_jobs=num_jobs, verbose=10)(delayed(compute_similarities)(chunk, index) for index, chunk in enumerate(np.array_split(tfidf_matrix_a, len(df_vendor_customer)//chunk_size+1)))\n",
    "# https://stackoverflow.com/questions/55955330/printed-output-not-displayed-when-using-joblib-in-jupyter-notebook --> cannot print the actual distribution to each CPU core\n",
    "\n",
    "\n",
    "# alternative solution but did not succeed, cannot view the Diagostic Dashboard on my web, and may due to no dask-env\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    results = Parallel(verbose=10)(delayed(compute_similarities)(chunk, index) for index, chunk in enumerate(np.array_split(tfidf_matrix_a, len(df_vendor_customer)//chunk_size+1)))\n",
    "print('End: Parallel Processing', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
